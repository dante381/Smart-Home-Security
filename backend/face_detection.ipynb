{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials, messaging\n",
    "\n",
    "firebase_cred = credentials.Certificate(\"smarthomesecurity-3e445-firebase-adminsdk-5w238-9ab3a99e6e.json\")\n",
    "firebase_app = firebase_admin.initialize_app(firebase_cred)\n",
    "\n",
    "message = messaging.Message(\n",
    "    notification=messaging.Notification(\n",
    "        title=\"SmartHomeSecurity\",\n",
    "        body=\"Unknown Person Detected\"\n",
    "    ),\n",
    "    token=\"dyRJGmXyQf-Rs7iARH1WYj:APA91bGvHZlumqH9448MaAMIafqob5YsFtrE_-8LnTWtaU_4iXZNnGFLrWx9Q0AVTmSMCeZZ9E1sY8TZZV548fsjHrqnie4XjZIdzGLKZokLxu2JBD8oh9804pDFfDFiUGXLmmuwvU5u\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\backend\\face_detection.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/yolov5surveillance-20230808T135321Z-001/backend/face_detection.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m strokes \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/yolov5surveillance-20230808T135321Z-001/backend/face_detection.ipynb#W1sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m cv\u001b[39m.\u001b[39mrectangle(frame, (x, y), (x\u001b[39m+\u001b[39mw, y\u001b[39m+\u001b[39mh), color, strokes)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Projects/yolov5surveillance-20230808T135321Z-001/backend/face_detection.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m dfs \u001b[39m=\u001b[39m DeepFace\u001b[39m.\u001b[39;49mfind(img_path\u001b[39m=\u001b[39;49mframe[y:y\u001b[39m+\u001b[39;49mh, x:x\u001b[39m+\u001b[39;49mw], db_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mE:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mProjects\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39myolov5surveillance-20230808T135321Z-001\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mbackend\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mDatabases\u001b[39;49m\u001b[39m'\u001b[39;49m, enforce_detection\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, detector_backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mopencv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/yolov5surveillance-20230808T135321Z-001/backend/face_detection.ipynb#W1sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# print(dfs) \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/yolov5surveillance-20230808T135321Z-001/backend/face_detection.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(dfs[\u001b[39m0\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m dfs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mloc[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mVGG-Face_cosine\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m):\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\deepface\\DeepFace.py:542\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(img_path, db_path, model_name, distance_metric, enforce_detection, detector_backend, align, normalization, silent)\u001b[0m\n\u001b[0;32m    539\u001b[0m resp_obj \u001b[39m=\u001b[39m []\n\u001b[0;32m    541\u001b[0m \u001b[39mfor\u001b[39;00m target_img, target_region, _ \u001b[39min\u001b[39;00m target_objs:\n\u001b[1;32m--> 542\u001b[0m     target_embedding_obj \u001b[39m=\u001b[39m represent(\n\u001b[0;32m    543\u001b[0m         img_path\u001b[39m=\u001b[39;49mtarget_img,\n\u001b[0;32m    544\u001b[0m         model_name\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[0;32m    545\u001b[0m         enforce_detection\u001b[39m=\u001b[39;49menforce_detection,\n\u001b[0;32m    546\u001b[0m         detector_backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mskip\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    547\u001b[0m         align\u001b[39m=\u001b[39;49malign,\n\u001b[0;32m    548\u001b[0m         normalization\u001b[39m=\u001b[39;49mnormalization,\n\u001b[0;32m    549\u001b[0m     )\n\u001b[0;32m    551\u001b[0m     target_representation \u001b[39m=\u001b[39m target_embedding_obj[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    553\u001b[0m     result_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcopy()  \u001b[39m# df will be filtered in each img\u001b[39;00m\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\deepface\\DeepFace.py:678\u001b[0m, in \u001b[0;36mrepresent\u001b[1;34m(img_path, model_name, enforce_detection, detector_backend, align, normalization)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39m# represent\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mkeras\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(model)):\n\u001b[0;32m    677\u001b[0m     \u001b[39m# new tf versions show progress bar and it is annoying\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     embedding \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(img, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    679\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    680\u001b[0m     \u001b[39m# SFace and Dlib are not keras models and no verbose arguments\u001b[39;00m\n\u001b[0;32m    681\u001b[0m     embedding \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(img)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py:2554\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2552\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2553\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2554\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[0;32m   2555\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2556\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    862\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 864\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    865\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    866\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32me:\\Projects\\yolov5surveillance-20230808T135321Z-001\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "from deepface import DeepFace\n",
    "import pandas as pd\n",
    "from imutils import video\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "\n",
    "face_cascade = cv.CascadeClassifier(\n",
    "    cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# cap = video.VideoStream('rtsp://192.168.1.30:8080/h264.sdp').start()\n",
    "cap=cv.VideoCapture(0)\n",
    "# cap.set(cv.CAP_PROP_FRAME_WIDTH, 160)\n",
    "# cap.set(cv.CAP_PROP_FRAME_HEIGHT, 120)\n",
    "\n",
    "count=0\n",
    "\n",
    "while True:\n",
    "    (grabbed,frame)= cap.read()\n",
    "    # frame = cap.read()\n",
    "    rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(rgb, scaleFactor=1.1, minNeighbors=5)\n",
    "    # frame = cv.resize(frame, (640, 480))\n",
    "    frame = cv.flip(frame, 1)\n",
    "    # print(frame.shape)\n",
    "    for (x, y, w, h) in faces:\n",
    "        color = (0, 255, 255)\n",
    "        strokes = 2\n",
    "\n",
    "        cv.rectangle(frame, (x, y), (x+w, y+h), color, strokes)\n",
    "        dfs = DeepFace.find(img_path=frame[y:y+h, x:x+w], db_path='E:\\\\Projects\\\\yolov5surveillance-20230808T135321Z-001\\\\backend\\\\Databases', enforce_detection=False, detector_backend=\"opencv\")\n",
    "        # print(dfs) \n",
    "        if (len(dfs[0]) == 0 or dfs[0].loc[0]['VGG-Face_cosine'] >= 0.1):\n",
    "            count+=1\n",
    "            if(count>10):\n",
    "                print(\"UNKNOWN PERSON DETECTED\")\n",
    "                count=0\n",
    "                messaging.send(message)\n",
    "        else:\n",
    "            count=0\n",
    "\n",
    "    # frame=cv.resize(frame,(600,400))\n",
    "    cv.imshow('Frame', frame)\n",
    "    key = cv.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv.waitKey(1)\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
